{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birdsong Classification Model\n",
    "by Beck, Carly, River, and Solomon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Importing Packages and Embedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# audio stuff\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2Processor, HubertModel\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# tensorflow and tings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "## clean up imports after :) ##\n",
    "# don't forget 'pip install -r requirements.txt'\n",
    "\n",
    "\n",
    "# the usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# audio stuff\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, HubertModel\n",
    "\n",
    "\n",
    "# tensorflow and tings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# sklearn tings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of beck's scribbles for inputting audio data and getting embeddings from that (using HuBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata from CSV with columns \"audio_file_path\" and \"label\"\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "\n",
    "# Initialize HuBERT model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "\n",
    "# # Function to extract MFCC features from an audio file\n",
    "# # We can come back to this if we need more information? I think HuBERT is better.\n",
    "# def extract_mfcc_tensor(audio_path):\n",
    "#     # Load the audio file\n",
    "#     audio, sr = librosa.load(audio_path, sr=None)\n",
    "#     # Extract MFCC features\n",
    "#     mfccs = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "#     return torch.tensor(mfccs, dtype=torch.float32)  # convert to a tensor\n",
    "\n",
    "def extract_hubert_embedding_tensor(audio_path):\n",
    "    # Load the audio file as a numpy array\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)  # ensure sample rate is 16000 for HuBERT because the internet said so\n",
    "    # Preprocess the audio file for HuBERT\n",
    "    input_values = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_values\n",
    "    # Pass input values through HuBERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    # Extract the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)  # keep as a 2D tensor (sequence_len, feature_dim)\n",
    "    return embeddings\n",
    "\n",
    "# Create the data list as a list of tuples\n",
    "data = [\n",
    "    (\n",
    "        # extract_mfcc_tensor(row['audio_file_path']),\n",
    "        extract_hubert_embedding_tensor(row['audio_file_path']),\n",
    "        torch.tensor(row['label'], dtype=torch.long)\n",
    "    )\n",
    "    for _, row in metadata.iterrows()\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preprocessing\n",
    "Here, we should:\n",
    "- shape our data so we can input it into the model\n",
    "- ensure all data is the same shape and size \n",
    "- print the size and shape of each class before and after reshaping (for bugtesting purposes)\n",
    "\n",
    "Consider [this link](https://pr454nt.medium.com/audio-embeddings-a-short-guide-for-understanding-audio-signals-in-vector-space-0e6551e50747#:~:text=Audio%20embeddings%20represent%20a%20groundbreaking,known%20as%20a%20vector%20space.) for embedding knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use this to pad the embeddings to the max length in our data\n",
    "\n",
    "# Define the collate function for 2D flattening\n",
    "def collate_fn_2d(batch):\n",
    "    # Find the maximum sequence length in this batch\n",
    "    max_seq_len = max([item.size(0) for item in batch])  # `item.size(0)` gives sequence length\n",
    "    \n",
    "    # Pad each sequence to the maximum length and flatten to 2D\n",
    "    padded_batch = [\n",
    "        torch.cat([item, torch.zeros(max_seq_len - item.size(0), item.size(1))], dim=0).view(-1)\n",
    "        for item in batch\n",
    "    ]\n",
    "    \n",
    "    # Stack all flattened embeddings into a batch tensor\n",
    "    padded_batch = torch.stack(padded_batch)  # shape: (batch_size, max_seq_len * feature_dim)\n",
    "    \n",
    "    return padded_batch, padded_batch  # Return input and target as the same for autoencoder or classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll make a dataloader because audio files are big and we don't want to load them all at once\n",
    "# we'll utilize this during training\n",
    "dataloader = DataLoader(data, batch_size=32, shuffle=True, collate_fn=collate_fn_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Visualizing Our Data\n",
    "Here, we should:\n",
    "- check class distribution of the full dataset\n",
    "- split the data into training, validation, and testing sets\n",
    "- check the class distribution of the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is a class distribution plotting function I used in text classification - Beck ###\n",
    "\n",
    "def plot_class_distribution(y, title, x_label='Class', y_label='Count', ax=None, y_max_override=None):\n",
    "    # Dynamically set y_max based on the current input data\n",
    "    y_max = y_max_override if y_max_override else max(Counter(y).values()) * 1.2\n",
    "\n",
    "    sns.countplot(x=y, palette='Accent', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label, size=12, color='grey')\n",
    "    ax.set_xticklabels(['Hate Speech', 'Offensive Language', 'Neither'])\n",
    "    ax.set_ylabel(y_label, size=12, color='grey')\n",
    "    ax.grid()\n",
    "\n",
    "    # Set y-axis limit\n",
    "    ax.set_ylim(0, y_max)\n",
    "\n",
    "    # Add percentage text on top of the bars\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width()/2, height + (y_max * 0.025), \n",
    "                f'{height/len(y):.2%}', ha='center', color='black', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the whole dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the whole dataset into training (to train), validation (to check training), and testing (to present results) sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of our subsets\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
