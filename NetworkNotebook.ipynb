{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birdsong Classification Model\n",
    "by Beck, Carly, River, and Solomon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up imports after :) ##\n",
    "# don't forget 'pip install -r requirements.txt'\n",
    "\n",
    "\n",
    "# the usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# audio stuff\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, HubertModel\n",
    "\n",
    "\n",
    "# tensorflow and tings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# sklearn tings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data <3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Splitting and Visualizing Our Data\n",
    "Here, we should:\n",
    "- check class distribution of the full dataset\n",
    "- split the data into training, validation, and testing sets\n",
    "- check the class distribution of the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is a class distribution plotting function I used for text classification - Beck ###\n",
    "\n",
    "def plot_class_distribution(y, title, x_label='Class', y_label='Count', ax=None, y_max_override=None):\n",
    "    # Dynamically set y_max based on the current input data\n",
    "    y_max = y_max_override if y_max_override else max(Counter(y).values()) * 1.2\n",
    "\n",
    "    sns.countplot(x=y, palette='Accent', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label, size=12, color='grey')\n",
    "    ax.set_xticklabels(['Hate Speech', 'Offensive Language', 'Neither'])\n",
    "    ax.set_ylabel(y_label, size=12, color='grey')\n",
    "    ax.grid()\n",
    "\n",
    "    # Set y-axis limit\n",
    "    ax.set_ylim(0, y_max)\n",
    "\n",
    "    # Add percentage text on top of the bars\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width()/2, height + (y_max * 0.025), \n",
    "                f'{height/len(y):.2%}', ha='center', color='black', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the class distribution of the whole dataset\n",
    "class_counts = metadata['label'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar', color='yellow')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the whole dataset into training and testing sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of our subsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Developing the Model\n",
    "Here, we should decide on and explain our CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set (copied from other code - Beck)\n",
    "\n",
    "hist = model.history.history    # rename training history for better quality of life\n",
    "\n",
    "# plot of accuracy during training\n",
    "plt.plot(hist['accuracy'], label='Training Accuracy')\n",
    "plt.plot(hist['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot of loss during training\n",
    "plt.plot(hist['loss'], label='Training Loss')\n",
    "plt.plot(hist['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present more evaluation metrics (copied from other code - Beck)\n",
    "\n",
    "y_test_pred_prob = model.predict(X_test)    # get the model's predictions for the test data (in probabilities)\n",
    "y_test_pred = np.argmax(y_test_pred_prob, axis=1)    # convert the predictions to class labels\n",
    "\n",
    "# calculate and print simpler evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Final Evaluation Metrics (Normalized)')\n",
    "print(f'Accuracy: {accuracy*100:.0f}')\n",
    "print(f'Precision: {precision*100:.0f}')\n",
    "print(f'Recall: {recall*100:.0f}')\n",
    "print(f'F1 Score: {f1*100:.0f}')\n",
    "\n",
    "# create and plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize='true') * 100  # normalize the confusion matrix and multiply by 100 to show percentages\n",
    "cm_labels = ['Hate Speech', 'Offensive Language', 'Neither']\n",
    "display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cm_labels)\n",
    "display.plot(cmap='Blues', values_format='.2f') # add color map and format to 2 decimal places\n",
    "plt.title(\"Normalized Confusion Matrix (Percentages)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
