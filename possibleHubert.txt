# Let's set up a folder of all of our audio files inside of the repo
# Load metadata from a CSV with columns "audio_file_path" and "label"
metadata = pd.read_csv('metadata.csv')

# Initialize HuBERT's model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/hubert-large-ls960-ft")
model = HubertModel.from_pretrained("facebook/hubert-large-ls960-ft")

# # Function to extract MFCC features from an audio file
# # We can come back to this if we need more information? I think HuBERT is better.
# def extract_mfcc_tensor(audio_path):
#     # Load the audio file
#     audio, sr = librosa.load(audio_path, sr=None)
#     # Extract MFCC features
#     mfccs = librosa.feature.mfcc(y=audio, sr=sr)
#     return torch.tensor(mfccs, dtype=torch.float32)  # convert to a tensor

def extract_hubert_embedding_tensor(audio_path):
    # Load the audio file as a numpy array
    audio, sr = librosa.load(audio_path, sr=16000)  # ensure sample rate is 16000 for HuBERT because the internet said so
    # Preprocess the audio file for HuBERT
    input_values = processor(audio, sampling_rate=sr, return_tensors="pt").input_values
    # Pass input values through HuBERT model
    with torch.no_grad():
        outputs = model(input_values)
    # Extract the embeddings from the last hidden state
    embeddings = outputs.last_hidden_state.squeeze(0)  # keep as a 2D tensor (sequence_len, feature_dim)
    return embeddings

# Create the data list as a list of tuples
data = [
    (
        # extract_mfcc_tensor(row['audio_file_path']),
        extract_hubert_embedding_tensor(row['audio_file_path']),
        torch.tensor(row['label'], dtype=torch.long)
    )
    for _, row in metadata.iterrows()
]



# we'll use this to pad the embeddings to the max length in our data

# Define the collate function for 2D flattening
def collate_fn_2d(batch):
    # Find the maximum sequence length in this batch
    max_seq_len = max([item.size(0) for item in batch])  # `item.size(0)` gives sequence length
    
    # Pad each sequence to the maximum length and flatten to 2D
    padded_batch = [
        torch.cat([item, torch.zeros(max_seq_len - item.size(0), item.size(1))], dim=0).view(-1)
        for item in batch
    ]
    
    # Stack all flattened embeddings into a batch tensor
    padded_batch = torch.stack(padded_batch)  # shape: (batch_size, max_seq_len * feature_dim)
    
    return padded_batch, padded_batch  # Return input and target as the same for autoencoder or classification

# and we'll make a dataloader because audio files are big and we don't want to load them all at once
# we'll utilize this during training
dataloader = DataLoader(data, batch_size=32, shuffle=True, collate_fn=collate_fn_2d)


